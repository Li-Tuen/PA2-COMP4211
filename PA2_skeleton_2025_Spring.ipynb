{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Li-Tuen/PA2-COMP4211/blob/main/PA2_skeleton_2025_Spring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports / Data Preparation"
      ],
      "metadata": {
        "id": "E1E-UUv8Gd4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np2WjNSfSzZr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "import tarfile\n",
        "import random\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To safely store your training progress, use Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Acq9ktkl4nWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Xubq35uEeIB"
      },
      "outputs": [],
      "source": [
        "tf.device('/GPU:0')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download CIFAR-10 dataset from the source:"
      ],
      "metadata": {
        "id": "sBNlOijxGZv3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAmbG7IVSoG9"
      },
      "outputs": [],
      "source": [
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "!tar -xzvf cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "NWO8aRGeMLar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMumV3wPSHqc"
      },
      "outputs": [],
      "source": [
        "def load_cifar10_batch(batch_filename):\n",
        "    with open(batch_filename, 'rb') as f:\n",
        "        dict = pickle.load(f, encoding='bytes')\n",
        "        X = dict[b'data']\n",
        "        Y = dict[b'labels']\n",
        "        X = X.reshape(-1, 3, 32, 32).astype(\"float32\")\n",
        "        X = np.transpose(X, (0, 2, 3, 1))  # Convert to NHWC\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "def load_cifar10(data_dir):\n",
        "    X_train = []\n",
        "    Y_train = []\n",
        "    # There are 5 training batches\n",
        "    for i in range(1, 6):\n",
        "        batch_file = os.path.join(data_dir, f'data_batch_{i}')\n",
        "        X, Y = load_cifar10_batch(batch_file)\n",
        "        X_train.append(X)\n",
        "        Y_train.append(Y)\n",
        "    X_train = np.concatenate(X_train)\n",
        "    Y_train = np.concatenate(Y_train)\n",
        "    # Load test batch\n",
        "    X_test, Y_test = load_cifar10_batch(os.path.join(data_dir, 'test_batch'))\n",
        "    return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "# Specify the path to the extracted CIFAR-10 data\n",
        "data_dir = 'cifar-10-batches-py'  # Change this path if different\n",
        "\n",
        "# Load the data\n",
        "X_train, Y_train, X_test, Y_test = load_cifar10(data_dir)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}, Training labels shape: {Y_train.shape}\")\n",
        "print(f\"Test data shape: {X_test.shape}, Test labels shape: {Y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the dataset\n",
        "def preprocess_data(X, Y):\n",
        "    X = X / 255.0  # Normalize to [0,1]\n",
        "    X = X.astype(np.float32)\n",
        "    Y = Y.astype(np.int32)\n",
        "    return X, Y\n",
        "\n",
        "X_train_p, Y_train_p = preprocess_data(X_train, Y_train)\n",
        "X_test_p, Y_test_p = preprocess_data(X_test, Y_test)\n"
      ],
      "metadata": {
        "id": "TxILlbAPMg7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Important constants/hyperparameters"
      ],
      "metadata": {
        "id": "WLUAVaElMVY9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some constants/information about the dataset:"
      ],
      "metadata": {
        "id": "HncJbThOG2Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "learning_rate = 1e-4"
      ],
      "metadata": {
        "id": "yMTyuvmdMahX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_p, Y_train_p))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=50000).batch(batch_size)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_p, Y_test_p))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "pL14c8PVd_pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_shape = X_train.shape[1:] # Shape of CIFAR-10 images\n",
        "image_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "n_class = len(image_labels)  # Number of classes"
      ],
      "metadata": {
        "id": "J1sajOSIGWuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsRRarlhsmB7"
      },
      "source": [
        "### Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBgecospS7fF"
      },
      "outputs": [],
      "source": [
        "# [Q1]\n",
        "def visualize_samples(X, Y, num_samples=16):\n",
        "\n",
        "    # Implement code here\n",
        "\n",
        "# Visualize 16 training samples\n",
        "visualize_samples(X_train_p, Y_train_p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74P7xs3osp4j"
      },
      "source": [
        "## Classification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9R0YTcEswc3"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [C1]\n",
        "class ResBlock(tf.keras.Model):\n",
        "    def __init__(self, in_channels : int, out_channels : int, downsampling = False):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n"
      ],
      "metadata": {
        "id": "Rou98Qk2Yu0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [C2]\n",
        "class WRN(tf.keras.Model):\n",
        "    def __init__(self, num_classes : int = 10):\n",
        "        super(WRN, self).__init__()\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "    def call(self, x):\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # ======================="
      ],
      "metadata": {
        "id": "iivps4l_9Oow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1 Training"
      ],
      "metadata": {
        "id": "GCO5b5TajtjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIIMiXoEeniI"
      },
      "outputs": [],
      "source": [
        "# [C3]\n",
        "def part1_train_step(optim : optimizers.Optimizer, model : tf.keras.Model, data : tf.Tensor, label : tf.Tensor, sigma : float = 0.03) -> dict:\n",
        "\n",
        "    # implement code here\n",
        "    # preprocessing\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "    # train model\n",
        "    with tf.GradientTape() as g:\n",
        "\n",
        "        # implement code here\n",
        "        # calculate loss here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "    # implement code here\n",
        "    # obtain the gradients and apply them (using optimizer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # =======================\n",
        "\n",
        "    # implement code here\n",
        "    # return loss (or other values if needed)\n",
        "\n",
        "\n",
        "    # =======================\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_1(model, optimizer, train_step, epochs : int = 20, save_interval : int = 5):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Wrap the training dataset with tqdm to create a progress bar\n",
        "        with tqdm(train_dataset, unit=\"batch\") as tepoch:\n",
        "            for step, (batch_x, batch_y) in enumerate(tepoch):\n",
        "                # Execute a train step and get the losses\n",
        "                loss_dict = train_step(optimizer, model, batch_x, batch_y)\n",
        "                epoch_loss += loss_dict[\"loss\"]\n",
        "\n",
        "                # Accumulate number of batches (for average loss calculation)\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update the progress bar description with the current loss\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                tepoch.set_postfix(**loss_dict)\n",
        "\n",
        "        # # Uncomment to test accuracy during training (Implement that first!)\n",
        "        test_accuracy = evaluate_accuracy(model, test_dataset)\n",
        "        print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Feel free to modify this to save your progress somewhere else\n",
        "        if epoch % save_interval == 0:\n",
        "            model.save_weights(f'model-{epoch}.weights.h5')\n"
      ],
      "metadata": {
        "id": "Y_uRSBlisPpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "PwnOHsR_uYex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [C4]\n",
        "def evaluate_accuracy(model, dataset):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Wrap the dataset with tqdm to create a progress bar\n",
        "    with tqdm(dataset, unit=\"batch\") as tepoch:\n",
        "        tepoch.set_description(f\"Evaluating\")\n",
        "        for batch_x, batch_y in tepoch:\n",
        "            batch_size = tf.shape(batch_x)[0]\n",
        "\n",
        "            # implement code here\n",
        "            # Compute logits\n",
        "\n",
        "\n",
        "\n",
        "            # =======================\n",
        "\n",
        "            # implement code here\n",
        "            # Cast batch_y to int64 to match preds data type\n",
        "\n",
        "\n",
        "\n",
        "            # =======================\n",
        "\n",
        "            # implement code here\n",
        "            # Update the progress bar description\n",
        "\n",
        "\n",
        "\n",
        "            # =======================\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "N560o54Guelt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Q5] Report the accuracy of the test dataset and show at least 4 misclassified samples"
      ],
      "metadata": {
        "id": "aeexDHuI1XVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute training"
      ],
      "metadata": {
        "id": "jOZxyEsmNY2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = WRN(num_classes=n_class)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "ui5RbFaZNb34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop_1(model, optimizer, part1_train_step, 20)"
      ],
      "metadata": {
        "id": "3yjnAJUANm26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regularization"
      ],
      "metadata": {
        "id": "OHkESeyubHeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [C5] Implement your regularization method here\n",
        "\n",
        "class ResBlockRE(tf.keras.Model):\n",
        "    def __init__(self, in_channels : int, out_channels : int, downsampling = False):\n",
        "        super(ResBlockRE, self).__init__()\n",
        "\n",
        "\n",
        "        # implement code here\n",
        "        # implement the methods by modifying this class\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "class WRNRE(tf.keras.Model):\n",
        "    def __init__(self, num_classes : int = 10):\n",
        "        super(WRNRE, self).__init__()\n",
        "\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "    def call(self, x):\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "\n",
        "        # =======================\n"
      ],
      "metadata": {
        "id": "yXatrgTaZini"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model_re = WRNRE(num_classes=n_class)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer_re = optimizers.Adam(learning_rate=learning_rate)\n"
      ],
      "metadata": {
        "id": "EWArsu-3a5Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop_1(model_re, optimizer_re, part1_train_step, 20)\n"
      ],
      "metadata": {
        "id": "Z29UJQlHa5k4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Q6] Report the accuracy of the test dataset after implementing regularization methods"
      ],
      "metadata": {
        "id": "7-zgJerSa55h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Generation Task"
      ],
      "metadata": {
        "id": "zyJbv065-JPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Energy Function"
      ],
      "metadata": {
        "id": "Ssx9zTA6DbDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [C6]\n",
        "def energy(model : tf.keras.Model, data : tf.Tensor, label : tf.Tensor | None = None):\n",
        "  # implement code here\n",
        "\n",
        "  # =======================\n"
      ],
      "metadata": {
        "id": "X-SeBxuBDfgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_energy(model: tf.keras.Model, sample_index: int = 20):\n",
        "    # Sample an image from the test dataset\n",
        "    # implement code here\n",
        "\n",
        "    # =======================\n",
        "    images = [real_image, noise_image, grey_image]\n",
        "    exy = []\n",
        "    ex = []\n",
        "    names = ['Real', 'Noise', 'Grey']\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "    for i, ax in enumerate(names):\n",
        "        exy.append(energy(model, tf.expand_dims(images[i], 0), real_label).numpy()[0])\n",
        "        ex.append(energy(model, tf.expand_dims(images[i], 0)).numpy()[0])\n",
        "        axes[i].imshow(images[i])\n",
        "        axes[i].set_title(f\"{ax} E(x,y): {exy[i]:.2f}, E(x) {ex[i]:.2f}\")\n",
        "        axes[i].axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4BU_uv29dBdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# [Q7]\n",
        "visualize_energy(model, 20)"
      ],
      "metadata": {
        "id": "8fWDtClDkBJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOQCGyQEsxVY"
      },
      "source": [
        "### Sampling x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfgQOTl6c2EA"
      },
      "outputs": [],
      "source": [
        "# [C7]\n",
        "def sampling_step(model : tf.keras.Model, data : tf.Tensor, label : tf.Tensor | None = None, step_size : float = 1, noise_amp : float = 0.01):\n",
        "    # This forces the gradienttape to only track gradients for the input data:\n",
        "    with tf.GradientTape(watch_accessed_variables = False) as g:\n",
        "        g.watch(data)\n",
        "        # calculate energy here (aggregate with sum)\n",
        "        # implement code here\n",
        "\n",
        "        # =======================\n",
        "\n",
        "\n",
        "    # implement code here\n",
        "\n",
        "    # calculate the gradient\n",
        "\n",
        "\n",
        "    # update data with gradient + gaussian noise\n",
        "\n",
        "    # =======================\n",
        "\n",
        "\n",
        "    # ensure that the sample is in the valid pixel space\n",
        "    data = tf.clip_by_value(data, 0, 1)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replay Buffer"
      ],
      "metadata": {
        "id": "EqofDCVEGFuE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb8AhNabkNMv"
      },
      "outputs": [],
      "source": [
        "# [C8]\n",
        "class SampleBuffer:\n",
        "    \"\"\" A buffer storing some past trained (negative) samples. \"\"\"\n",
        "    def __init__(self, sample_shape, n_class : int, max_samples : int = 10000, dtype : np.dtype = np.float32):\n",
        "        self.max_samples = max_samples\n",
        "        self.sample_shape = sample_shape\n",
        "        self.n_class = n_class\n",
        "        self.dtype = dtype\n",
        "        self.buffer = []\n",
        "\n",
        "    def add_to_buffer(self, samples, ids):\n",
        "        \"\"\" Add samples to replay buffer (self.buffer). If there are too many samples in the buffer, remove the oldest ones (first in, first out). \"\"\"\n",
        "        # implement code here\n",
        "\n",
        "\n",
        "        # =======================\n",
        "\n",
        "    def sample_from_buffer(self, n_samples : int, p_new : float = 0.05):\n",
        "        \"\"\" Sample batch of n_sample samples, with each sample being either new (with probability p_new) or from buffer.\"\"\"\n",
        "        if len(self.buffer) == 0:\n",
        "            # Initial case\n",
        "            n_new = n_samples\n",
        "        else:\n",
        "            n_new = np.random.binomial(n_samples, p_new)\n",
        "\n",
        "        if n_new > 0:\n",
        "            # Sample uniform random data\n",
        "            # implement code here\n",
        "\n",
        "            # =======================\n",
        "        else:\n",
        "            noise, noise_class = [], []\n",
        "\n",
        "        if n_new < n_samples:\n",
        "            # Sample from existing data\n",
        "            # implement code here\n",
        "\n",
        "            # =======================\n",
        "        else:\n",
        "            replay, replay_class = [], []\n",
        "\n",
        "        sample = tf.stack(list(noise) + list(replay), axis = 0)\n",
        "        sample_class = tf.stack(list(noise_class) + list(replay_class), axis = 0)\n",
        "\n",
        "        return sample, sample_class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_buffer_samples(buffer : SampleBuffer, num_samples : int = 16, p_new : float = 0.05):\n",
        "    # Retrieve samples and their corresponding labels from the buffer\n",
        "    samples, labels = buffer.sample_from_buffer(num_samples, p_new)\n",
        "    # Visualize the samples\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(num_samples):\n",
        "        plt.subplot(4, 4, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(samples[i].numpy())  # Convert to uint8 for proper image display\n",
        "        plt.xlabel(image_labels[labels[i].numpy()])\n",
        "    plt.show()\n",
        "\n",
        "sample_shape = X_train_p[0].shape\n",
        "buffer = SampleBuffer(sample_shape, n_class)\n",
        "# Add some initial data to the buffer\n",
        "buffer.add_to_buffer(X_train_p[:100], Y_train_p[:100])  # Add first 100 samples as an example"
      ],
      "metadata": {
        "id": "9KAXPeUj5nx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#[Q8]\n",
        "visualize_buffer_samples(buffer, num_samples=16, p_new=0.5)"
      ],
      "metadata": {
        "id": "KDBf-0rR5ZxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JEM Training"
      ],
      "metadata": {
        "id": "-ck9A1F_jkRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[C9]\n",
        "def part2_train_step(optim : optimizers.Optimizer, model : tf.keras.Model, data : tf.Tensor, label : tf.Tensor, sb : SampleBuffer, sigma : float = 0.03) -> dict:\n",
        "    batch_size = len(data)\n",
        "    # preprocessing\n",
        "    x, y = (data + tf.random.normal(data.shape) * sigma), tf.one_hot(label, model.num_classes)\n",
        "\n",
        "    # implement code here\n",
        "\n",
        "    # sample images and labels (x', y') from SampleBuffer\n",
        "\n",
        "    # run sampling step on x' 20 times, then add the final result back to the buffer\n",
        "\n",
        "    # =======================\n",
        "\n",
        "\n",
        "    # train model\n",
        "    with tf.GradientTape() as g:\n",
        "        # implement code here\n",
        "\n",
        "        # calculate loss here, by:\n",
        "        # 1. obtainig model output for x and x'\n",
        "\n",
        "        # 2. calculate cross-entropy and energy values\n",
        "\n",
        "        # 3. aggregate and calculate the final loss using the mentioned values\n",
        "\n",
        "        # =======================\n",
        "    # obtain the gradients and apply them (using optimizer)\n",
        "    centry = tf.reduce_mean(centry)\n",
        "    grad = g.gradient(loss, model.trainable_variables)\n",
        "    optim.apply_gradients(zip(grad, model.trainable_variables))\n",
        "\n",
        "    # return loss (or other values if needed)\n",
        "    return {\n",
        "        \"loss\": loss.numpy(),\n",
        "        \"class_loss\": centry.numpy()\n",
        "    }"
      ],
      "metadata": {
        "id": "UYZI6xYPt56J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop_2(model, optimizer, train_step, epochs : int = 5, save_interval : int = 1, buffer : SampleBuffer | None = None) -> SampleBuffer:\n",
        "    if buffer is None:\n",
        "        buffer = SampleBuffer(X_train[0].shape, n_class)\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Wrap the training dataset with tqdm to create a progress bar\n",
        "        with tqdm(train_dataset, unit=\"batch\") as tepoch:\n",
        "            for step, (batch_x, batch_y) in enumerate(tepoch):\n",
        "                # Execute a train step and get the losses\n",
        "                loss_dict = train_step(optimizer, model, batch_x, batch_y, buffer)\n",
        "                epoch_loss += loss_dict[\"loss\"]\n",
        "\n",
        "                # Accumulate number of batches (for average loss calculation)\n",
        "                num_batches += 1\n",
        "\n",
        "                # Update the progress bar description with the current loss\n",
        "                tepoch.set_description(f\"Epoch {epoch}\")\n",
        "                tepoch.set_postfix(**loss_dict)\n",
        "\n",
        "        # Uncomment to test accuracy during training (Implement that first!)\n",
        "        test_accuracy = evaluate_accuracy(model, test_dataset)\n",
        "        print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        print(f\"Epoch {epoch}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Feel free to modify this to save your progress somewhere else\n",
        "        if epoch % save_interval == 0:\n",
        "            model.save_weights(f'model-{epoch}.weights.h5')\n",
        "\n",
        "    return buffer"
      ],
      "metadata": {
        "id": "CQqWyTa-nKHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute training"
      ],
      "metadata": {
        "id": "R1ZzWR4BOC9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model_2 = WRN(num_classes=n_class)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer_2 = optimizers.Adam(learning_rate=learning_rate)"
      ],
      "metadata": {
        "id": "2NJpLWKbOFyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the training diverges, please restart the training"
      ],
      "metadata": {
        "id": "clwHTECqyubw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buf = train_loop_2(model_2, optimizer_2, part2_train_step, 2)"
      ],
      "metadata": {
        "id": "NpJ9qN0b0LJD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}